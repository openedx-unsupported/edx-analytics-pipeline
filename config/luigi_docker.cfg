[job-conf]
# The third-party docker containers we use run everything as "root". Setting this environment variable on all child
# processes allows us to write to HDFS as the hadoop user instead. Otherwise we end up writing a mix of "hadoop" user
# owned files and "root" owned files, which are often not readable by the "hadoop" user.
mapred.child.env = HADOOP_USER_NAME=hadoop

[retcode]
# The following return codes are the recommended exit codes for Luigi.
# They are in increasing level of severity (for most applications).
already_running = 10
missing_data = 20
not_run = 25
task_failed = 30
scheduling_error = 35
unhandled_exception = 40

[hive]
release = apache
version = 1.0
database = default
warehouse_path = hdfs://namenode:8020/edx-analytics-pipeline/warehouse/

[database-export]
database = reports
credentials = /edx/etc/edx-analytics-pipeline/output.json


[database-import]
database = edxapp
credentials = /edx/etc/edx-analytics-pipeline/input.json
destination = hdfs://namenode:8020/edx-analytics-pipeline/warehouse/

[otto-database-import]
database = ecommerce
credentials = /edx/etc/edx-analytics-pipeline/input.json

[map-reduce]
engine = hadoop
marker = hdfs://namenode:8020/edx-analytics-pipeline/marker/

[event-logs]
pattern = [".*tracking.log.*"]
expand_interval = 2 days
source = ["hdfs://namenode:8020/data/"]

[event-export]
output_root = hdfs://namenode:8020/edx-analytics-pipeline/event-export/output/
environment = simple
config = hdfs://namenode:8020/edx-analytics-pipeline/event_export/config.yaml
gpg_key_dir = hdfs://namenode:8020/edx-analytics-pipeline/event_export/gpg-keys/
gpg_master_key = master@key.org
required_path_text = FakeServerGroup

[event-export-course]
output_root = hdfs://namenode:8020/edx-analytics-pipeline/event-export-by-course/output/

[obfuscation]
output_root = hdfs://namenode:8020/edx-analytics-pipeline/obfuscation/output/
explicit_event_whitelist = explicit_events.tsv
xblock_obfuscation_config = xblock_obfuscation_config.yml

[id-codec]
seed_value = 42

[manifest]
threshold = 500
input_format = org.edx.hadoop.input.ManifestTextInputFormat
lib_jar = hdfs://namenode:8020/edx-analytics-pipeline/packages/edx-analytics-hadoop-util.jar
path = hdfs://namenode:8020/edx-analytics-pipeline/manifest/

[user-activity]
output_root = hdfs://namenode:8020/edx-analytics-pipeline/activity/

[enrollments]
interval_start = 2013-11-01
overwrite_n_days = 2

[geolocation]
geolocation_data = hdfs://namenode:8020/edx-analytics-pipeline/geo.dat

[calendar]
interval = 2012-01-01-2020-01-01

[videos]
dropoff_threshold = 0.05

[elasticsearch]
host = ["http://elasticsearch:9200/"]

[module-engagement]
alias = roster
number_of_shards = 5

[ccx]
enabled = false

[problem-response]
report_fields = [
    "username",
    "problem_id",
    "answer_id",
    "location",
    "question",
    "score",
    "max_score",
    "correct",
    "answer",
    "total_attempts",
    "first_attempt_date",
    "last_attempt_date"]
report_output_root = /edx/src/problem-response-reports/

[edx-rest-api]
client_id = oauth_id
client_secret = oauth_secret
#auth_url = http://localhost:8000/oauth2/access_token/

#[course-list]
#api_root_url = http://localhost:8000/api/courses/v1/courses/

#[course-blocks]
#api_root_url = http://localhost:8000/api/courses/v1/blocks/

[spark]
master=spark://sparkmaster:7077
driver-memory=2g
executor-memory=3g
executor-cores=2

# Follow the documentation to create and install these egg files, if needed.
# You can also add other .egg or .zip files that you wish (or need) to prebuild.
# Opaque_keys and ccx_keys need to be prebuilt because they cannot easily be built at runtime
# to include metadata about their entry points, needed for opaque keys to work.
prebuilt_python_modules = {"opaque_keys": "/var/tmp/edx_egg_files/edx_opaque_keys.egg", "ccx_keys": "/var/tmp/edx_egg_files/edx_ccx_keys.egg"}
