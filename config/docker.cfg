# Settings that can be used when running inside of a docker container
[hadoop]
version = cdh4
command = /home/hadoop/hadoop/bin/hadoop
streaming-jar = /home/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.3.0.jar

[hive]
release = apache
database = default
warehouse_path = hdfs://localhost:9000/edx-analytics-pipeline/warehouse/

[database-export]
database = reports_2_0
credentials = /var/tmp/database.json

[database-import]
database = edxapp
credentials = /var/tmp/database.json
destination = hdfs://localhost:9000/edx-analytics-pipeline/warehouse/

[map-reduce]
engine = hadoop
marker = hdfs://localhost:9000/edx-analytics-pipeline/marker/

[event-logs]
pattern = .*tracking.log-(?P<date>\d{8}).*\.gz
expand_interval = 2 days
source = hdfs://localhost:9000/data/

[event-export]
output_root = hdfs://localhost:9000/edx-analytics-pipeline/event-export/output/
environment = simple
config = hdfs://localhost:9000/edx-analytics-pipeline/event_export/config.yaml
gpg_key_dir = hdfs://localhost:9000/edx-analytics-pipeline/event_export/gpg-keys/
gpg_master_key = master@key.org
required_path_text = FakeServerGroup

[manifest]
threshold = 500
input_format = org.edx.localhost.input.ManifestTextInputFormat
lib_jar = hdfs://localhost:9000/edx-analytics-pipeline/packages/edx-analytics-localhost-util.jar
path = hdfs://localhost:9000/edx-analytics-pipeline/manifest/

[user-activity]
output_root = hdfs://localhost:9000/edx-analytics-pipeline/activity/

[enrollments]
blacklist_date = 2014-08-18
blacklist_path = hdfs://localhost:9000/edx-analytics-pipeline/blacklist/

[enrollment-reports]
src = hdfs://localhost:9000/edx-analytics-pipeline/data/
destination = hdfs://localhost:9000/edx-analytics-pipeline/enrollment_reports/output/
offsets = hdfs://localhost:9000/edx-analytics-pipeline/enrollment_reports/offsets.tsv
blacklist = hdfs://localhost:9000/edx-analytics-pipeline/enrollment_reports/course_blacklist.tsv
history = hdfs://localhost:9000/edx-analytics-pipeline/enrollment_reports/enrollment_history.tsv

[geolocation]
geolocation_data = hdfs://localhost:9000/edx-analytics-pipeline/geo.dat

[calendar]
interval = 2012-01-01-2020-01-01
